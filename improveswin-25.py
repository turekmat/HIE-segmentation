# -*- coding: utf-8 -*-
"""improveswin

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/improveswin-7d05eee5-67c1-41d9-b208-5902b8a76585.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250314/auto/storage/goog4_request%26X-Goog-Date%3D20250314T170838Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7c309fdacb6e3287139b7113dcb36396e152d4260728625e4d3106b9fa41028383b3754946aa7c95ab84346421072c6470bf02c512656817543f1b152e3e498d1d3ae06e68040c218e06a210f82b2441ac98c3e8cd8c54e046e9d1f3c88aba232dfb7fff5bcf67b9ba830a59322ab782254193fba63aea15d37691a4cb9c17e5a62ac72c01c5befc0a53977a9720d893910dcd61601b5ddcc97a5ea20c4e4a51059e0fb695c3f5f077e7a4e13149d1d9acea0cc65c4d7a6906e0021f28f03948201276ecf8825f4912a665115b4febdf90c76de128aa856822838beb466e131442ad5b2e7312bbf62b3441ea22e66449005d40a9879a30756b6511d27bce7f80
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

turekmat_elastic_transform_bonbib_path = kagglehub.dataset_download('turekmat/elastic-transform-bonbib')
turekmat_nnunet_dataset_path = kagglehub.dataset_download('turekmat/nnunet-dataset')
turekmat_bonbid_2023_train_path = kagglehub.dataset_download('turekmat/bonbid-2023-train')

print('Data source import complete.')

"""fa6c31f3ce3a6ee5696c297845d56198b4d57ff3"""

!pip install --upgrade wandb
import wandb
import os

running_on = "kaggle" #kaggle, colab

if running_on == "kaggle":
    from kaggle_secrets import UserSecretsClient
    user_secrets = UserSecretsClient()
    secret_value_0 = user_secrets.get_secret("WANDB_API_KEY")


    if secret_value_0:
        wandb.login(key=secret_value_0)
        print("Successfully logged into Weights & Biases!")
    else:
        print("Error: WANDB_API_KEY not found in Kaggle Secrets. Please add it manually.")

else:
    wandb.login()
    from google.colab import drive
    drive.mount('/content/drive')

!pip install SimpleITK
!pip install --upgrade monai

import os
import numpy as np
import SimpleITK as sitk
import random
import wandb

import shutil
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split, Subset
import matplotlib.pyplot as plt

import monai
from monai.networks.nets import SwinUNETR
from monai.networks.blocks import PatchEmbed
from monai.inferers import sliding_window_inference
from monai.losses import DiceFocalLoss
from monai.networks.utils import one_hot

import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
from torch.utils.data import WeightedRandomSampler
from scipy.ndimage import rotate
from scipy.ndimage import distance_transform_edt, binary_erosion

alpha     = 0
bg_weight = 1
fg_weight = 40
epochs    = 15
lr        = 0.0008 #SwinBest 0.0008
eta_min = 4e-5 #SwinBest 4e-6
patch_size= (64,64,64)
step_size = 10
gamma     = 0.7
drop_rate = 0.15

use_data_augmentation = True
use_normalization = True
USE_TTA = True
TTA_ANGLE_MAX = 3

n_folds   = 5
batch_size = 1

model_name="SwinUNETR"  #SwinUNETR, UNet3Plus
in_channels = 2
out_channels = 2

loss_name = "focal_dice_combo" # weighted_ce_dice, log_cosh_dice, focal_tversky, log_hausdorff, focal, focal_dice_combo, focal_ce_combo, dice_focal

use_moe = False
expert_model_name="SwinUNETR"
expert_loss_name = "focal_dice_combo"
expert_lr=0.0004
expert_eta_min=1e-6
threshold_expert = 60

focal_alpha = 0.75
focal_gamma = 2.0
alpha_mix = 0.6

compute_surface_metrics = True
allow_normalize_spacing = True

extended_dataset = True
max_aug_per_orig = 0

use_ohem = False
ohem_ratio = 0.15

training_mode = "full_volume"  #patch, full_volume
patch_per_volume = 1

adc_folder = ""
z_folder = ""
label_folder = ""
preprocessed_adc_folder = ""
preprocessed_z_folder = ""
preprocessed_label_folder = ""
out_dir_random = ""

if running_on == "kaggle":
    out_dir_random = "/kaggle/working/inference"
    adc_folder   = "/kaggle/input/bonbid-2023-train/BONBID2023_Train/1ADC_ss"
    z_folder     = "/kaggle/input/bonbid-2023-train/BONBID2023_Train/2Z_ADC"
    label_folder = "/kaggle/input/bonbid-2023-train/BONBID2023_Train/3LABEL"
    if extended_dataset:
        preprocessed_adc_folder = "/kaggle/input/elastic-transform-bonbib/1ADC_ss"
        preprocessed_z_folder = "/kaggle/input/elastic-transform-bonbib/2Z_ADC"
        preprocessed_label_folder = "/kaggle/input/elastic-transform-bonbib/3LABEL"
    else:
        preprocessed_adc_folder = "/kaggle/working/preprocessed/1ADC_ss"
        preprocessed_z_folder = "/kaggle/working/preprocessed/2Z_ADC"
        preprocessed_label_folder = "/kaggle/working/preprocessed/3LABEL"

elif running_on == "colab":
    out_dir_random = "/content/drive/MyDrive/data/inference"
    adc_folder   = "/content/drive/MyDrive/data/BONBID2023_Train/1ADC_ss"
    z_folder     = "/content/drive/MyDrive/data/BONBID2023_Train/2Z_ADC"
    label_folder = "/content/drive/MyDrive/data/BONBID2023_Train/3LABEL"
    if extended_dataset:
        preprocessed_adc_folder = "/content/drive/MyDrive/archive-3/1ADC_ss"
        preprocessed_z_folder = "/content/drive/MyDrive/archive-3/2Z_ADC"
        preprocessed_label_folder = "/content/drive/MyDrive/archive-3/3LABEL"
    else:
        preprocessed_adc_folder = "/content/drive/MyDrive/data/preprocessed/BONBID2023_Train/1ADC_ss"
        preprocessed_z_folder = "/content/drive/MyDrive/data/preprocessed/BONBID2023_Train/2Z_ADC"
        preprocessed_label_folder = "/content/drive/MyDrive/data/preprocessed/BONBID2023_Train/3LABEL"

def create_model(model_name="SwinUNETR", in_channels=2, out_channels=2):
    if model_name == "SwinUNETR":
        model = SwinUNETR(
            img_size=(64, 64, 64),
            in_channels=in_channels,
            out_channels=out_channels,
            feature_size=24,
            use_checkpoint=False,
            spatial_dims=3,
            drop_rate = drop_rate,
        )
    elif model_name == "AttentionUNet":
        model = AttentionUNet(
          in_channels=in_channels,
          out_channels=out_channels)

    elif model_name == "UNet3Plus":
        model = UNet3Plus3D(
            in_channels=in_channels,
            out_channels=out_channels,
            base_channels=64
        )
    else:
        raise ValueError(f"Unsupported model_name: {model_name}")
    return model

import torch
import torch.nn as nn
import torch.nn.functional as F

def create_feature_maps(init_channel_number, number_of_fmaps):
    """
    Vygeneruje list velikostí feature map.
    Defaultně pro 6 úrovní (můžete přizpůsobit).
    """
    return [init_channel_number * 2 ** k for k in range(number_of_fmaps)]

class SCA3D(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SCA3D, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        self.channel_excitation = nn.Sequential(
            nn.Linear(channel, channel // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel)
        )
        self.spatial_se = nn.Conv3d(channel, 1, kernel_size=1, stride=1, padding=0, bias=False)

    def forward(self, x):
        b, c, d, h, w = x.size()
        chn_se = self.avg_pool(x).view(b, c)
        chn_se = torch.sigmoid(self.channel_excitation(chn_se))
        chn_se = chn_se.view(b, c, 1, 1, 1)
        x_channel_att = x * chn_se

        spa_se = torch.sigmoid(self.spatial_se(x))
        x_spatial_att = x * spa_se

        out = x + x_channel_att + x_spatial_att
        return out


def conv3d(in_channels, out_channels, kernel_size, bias, padding=1):
    return nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding, bias=bias)


def create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding=1):

    assert 'c' in order, "Konvoluce (c) musí být ve stringu order"
    assert order[0] not in 'rle', "Nejprve musí být konvoluce, pak až aktivace"

    modules = []
    for i, char in enumerate(order):
        if char == 'r':
            modules.append(('ReLU', nn.ReLU(inplace=True)))
        elif char == 'l':
            modules.append(('LeakyReLU', nn.LeakyReLU(negative_slope=0.1, inplace=True)))
        elif char == 'e':
            modules.append(('ELU', nn.ELU(inplace=True)))
        elif char == 'c':
            bias = not ('g' in order or 'b' in order)
            modules.append(('conv', conv3d(in_channels, out_channels, kernel_size, bias, padding=padding)))
        elif char == 'g':

            if out_channels < num_groups:
                num_groups = out_channels
            modules.append(('groupnorm', nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)))
        elif char == 'b':

            is_before_conv = i < order.index('c')
            if is_before_conv:
                modules.append(('batchnorm', nn.BatchNorm3d(in_channels)))
            else:
                modules.append(('batchnorm', nn.BatchNorm3d(out_channels)))
        else:
            raise ValueError(f"Neznámý znak vrstvy '{char}'. Povolené: ['b','g','r','l','e','c']")

    return modules


class SingleConv(nn.Sequential):
    def __init__(self, in_channels, out_channels, kernel_size=3, order='crg', num_groups=8, padding=1):
        super().__init__()
        ops = create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding=padding)
        for name, module in ops:
            self.add_module(name, module)


class DoubleConv(nn.Sequential):
    def __init__(
        self,
        in_channels,
        out_channels,
        encoder=True,
        kernel_size=3,
        order='crg',
        num_groups=8
    ):
        super().__init__()
        if encoder:
            # Zpravidla pro enkodér
            conv1_out = max(in_channels, out_channels // 2)
            self.add_module('SingleConv1', SingleConv(in_channels, conv1_out, kernel_size, order, num_groups))
            self.add_module('SingleConv2', SingleConv(conv1_out, out_channels, kernel_size, order, num_groups))
        else:
            # Dekodér
            self.add_module('SingleConv1', SingleConv(in_channels, out_channels, kernel_size, order, num_groups))
            self.add_module('SingleConv2', SingleConv(out_channels, out_channels, kernel_size, order, num_groups))


class Encoder(nn.Module):

    def __init__(
        self,
        in_channels,
        out_channels,
        conv_kernel_size=3,
        apply_pooling=True,
        pool_kernel_size=(2, 2, 2),
        pool_type='max',
        basic_module=DoubleConv,
        conv_layer_order='crg',
        num_groups=8
    ):
        super().__init__()
        assert pool_type in ['max', 'avg'], "pool_type musí být 'max' nebo 'avg'"

        if apply_pooling:
            if pool_type == 'max':
                self.pooling = nn.MaxPool3d(kernel_size=pool_kernel_size)
            else:
                self.pooling = nn.AvgPool3d(kernel_size=pool_kernel_size)
        else:
            self.pooling = None

        self.basic_module = basic_module(
            in_channels,
            out_channels,
            encoder=True,
            kernel_size=conv_kernel_size,
            order=conv_layer_order,
            num_groups=num_groups
        )

    def forward(self, x):
        if self.pooling is not None:
            x = self.pooling(x)
        x = self.basic_module(x)
        return x


class Decoder(nn.Module):
    def __init__(
        self,
        in_channels,
        out_channels,
        kernel_size=3,
        scale_factor=(2, 2, 2),
        basic_module=DoubleConv,
        conv_layer_order='crg',
        num_groups=8
    ):
        super().__init__()
        self.upsample = None  # v originále by šlo nastavit ConvTranspose...

        self.scse = SCA3D(in_channels)

        self.basic_module = basic_module(
            in_channels,
            out_channels,
            encoder=False,
            kernel_size=kernel_size,
            order=conv_layer_order,
            num_groups=num_groups
        )

    def forward(self, encoder_features, x):
        if self.upsample is None:
            # nearest neighbor upsampling
            out_size = encoder_features.size()[2:]  # D,H,W
            x = F.interpolate(x, size=out_size, mode='nearest')
            # skip: spojit enkodérové a dekód. feature mapy
            x = torch.cat((encoder_features, x), dim=1)
        else:
            # pokud byste chtěli ConvTranspose3d => x = self.upsample(x) ; x += encoder_features
            raise NotImplementedError("V této verzi se pro upsampling používá nearest+concat.")

        x = self.scse(x)
        x = self.basic_module(x)
        return x


class AttentionUNet(nn.Module):
    """
    3D U-Net s "attention" (SCA3D) v dekodéru.
    - in_channels: počet vstupních kanálů (např. 2 => [ADC, Z_ADC])
    - out_channels: počet výstupních kanálů (pro segmentaci => 2 => [BG, FG])
    - final_sigmoid: zda aplikovat Sigmoid na výstup (True pro binární, False -> Softmax)
    - f_maps: základní počet feature map (16, 32, 64, 128, 256, 512 atp.)
    - layer_order: např. "crg" => Conv + ReLU + GroupNorm
    - num_groups: pro GroupNorm
    """
    def __init__(
        self,
        in_channels=2,
        out_channels=2,
        final_sigmoid=False,
        f_maps=16,
        layer_order='crg',
        num_groups=8,
        **kwargs
    ):
        super().__init__()

        if isinstance(f_maps, int):
            # např. 16, 32, 64, 128, 256, 512 atd. (6 úrovní)
            f_maps = create_feature_maps(f_maps, number_of_fmaps=6)

        # --- ENCODERS ---
        encoders = []
        for i, out_feat_num in enumerate(f_maps):
            if i == 0:
                encoder = Encoder(
                    in_channels, out_feat_num,
                    apply_pooling=False,
                    basic_module=DoubleConv,
                    conv_layer_order=layer_order,
                    num_groups=num_groups
                )
            else:
                encoder = Encoder(
                    f_maps[i - 1], out_feat_num,
                    basic_module=DoubleConv,
                    conv_layer_order=layer_order,
                    num_groups=num_groups
                )
            encoders.append(encoder)
        self.encoders = nn.ModuleList(encoders)

        decoders = []
        reversed_f = list(reversed(f_maps))
        for i in range(len(reversed_f) - 1):
            in_chan = reversed_f[i] + reversed_f[i + 1]
            out_chan= reversed_f[i + 1]
            decoder = Decoder(
                in_channels=in_chan,
                out_channels=out_chan,
                basic_module=DoubleConv,
                conv_layer_order=layer_order,
                num_groups=num_groups
            )
            decoders.append(decoder)
        self.decoders = nn.ModuleList(decoders)

        self.final_conv = nn.Conv3d(f_maps[0], out_channels, kernel_size=1)

        if final_sigmoid:
            self.final_activation = nn.Sigmoid()
        else:
            self.final_activation = nn.Softmax(dim=1)

    def forward(self, x):
        """
        x shape: [B, in_channels, D, H, W]
        """
        encoder_features = []
        for encoder in self.encoders:
            x = encoder(x)
            encoder_features.insert(0, x)

        bottom = encoder_features[0]
        skip_list = encoder_features[1:]

        x = bottom
        for decoder, skip_f in zip(self.decoders, skip_list):
            x = decoder(skip_f, x)

        x = self.final_conv(x)

        if not self.training:
            x = self.final_activation(x)

        return x

class UNet3Plus3D(nn.Module):
    """
    A 3D adaptation of UNet 3+ for volume segmentation.
    This is a simplified version focusing on the full-scale skip connection
    to the highest (original) resolution only (i.e. 'D0' aggregator).
    """
    def __init__(
        self,
        in_channels=2,
        out_channels=2,
        base_channels=64
    ):
        """
        Args:
            in_channels  (int): number of input channels (e.g., 2 => [ADC, Z_ADC])
            out_channels (int): number of output channels (e.g., 2 => [BG, FG])
            base_channels(int): number of channels in the first UNet level.
                                (subsequent levels use multiples of this)
        """
        super(UNet3Plus3D, self).__init__()

        filters = [
            base_channels,
            base_channels * 2,
            base_channels * 4,
            base_channels * 8,
            base_channels * 16
        ]  # [64, 128, 256, 512, 1024] by default

        # --- Encoder ---
        self.enc0 = self._make_enc_block(in_channels, filters[0])   # level 0
        self.pool0 = nn.MaxPool3d(kernel_size=2, stride=2)

        self.enc1 = self._make_enc_block(filters[0], filters[1])    # level 1
        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)

        self.enc2 = self._make_enc_block(filters[1], filters[2])    # level 2
        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)

        self.enc3 = self._make_enc_block(filters[2], filters[3])    # level 3
        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)

        self.enc4 = self._make_enc_block(filters[3], filters[4])    # level 4 (deepest)

        self.conv0_0 = nn.Conv3d(filters[0], base_channels, kernel_size=3, padding=1)
        self.conv1_0 = nn.Conv3d(filters[1], base_channels, kernel_size=3, padding=1)
        self.conv2_0 = nn.Conv3d(filters[2], base_channels, kernel_size=3, padding=1)
        self.conv3_0 = nn.Conv3d(filters[3], base_channels, kernel_size=3, padding=1)
        self.conv4_0 = nn.Conv3d(filters[4], base_channels, kernel_size=3, padding=1)

        # BatchNorm + activation for aggregator outputs
        self.bn0_0 = nn.BatchNorm3d(base_channels)
        self.bn1_0 = nn.BatchNorm3d(base_channels)
        self.bn2_0 = nn.BatchNorm3d(base_channels)
        self.bn3_0 = nn.BatchNorm3d(base_channels)
        self.bn4_0 = nn.BatchNorm3d(base_channels)

        # Final conv after concatenation => combine 5*base_channels => 320 if base_channels=64
        self.agg_conv = nn.Sequential(
            nn.Conv3d(base_channels * 5, filters[0], kernel_size=3, padding=1),
            nn.BatchNorm3d(filters[0]),
            nn.ReLU(inplace=True)
        )

        # Final classification layer => out_channels
        self.out_conv = nn.Conv3d(filters[0], out_channels, kernel_size=1)

    def _make_enc_block(self, in_ch, out_ch):
        """
        Basic 3D U-Net encoder block: (Conv3D+BN+ReLU) x 2
        """
        return nn.Sequential(
            nn.Conv3d(in_ch, out_ch, kernel_size=3, padding=1),
            nn.BatchNorm3d(out_ch),
            nn.ReLU(inplace=True),

            nn.Conv3d(out_ch, out_ch, kernel_size=3, padding=1),
            nn.BatchNorm3d(out_ch),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        """
        x shape: [B, in_channels, D, H, W]
        """
        # --- Encoder downsampling ---
        x0 = self.enc0(x)             # -> (B, filters[0], D, H, W)
        x1 = self.enc1(self.pool0(x0))# -> (B, filters[1], D/2, H/2, W/2)
        x2 = self.enc2(self.pool1(x1))# -> (B, filters[2], D/4, H/4, W/4)
        x3 = self.enc3(self.pool2(x2))# -> (B, filters[3], D/8, H/8, W/8)
        x4 = self.enc4(self.pool3(x3))# -> (B, filters[4], D/16,H/16,W/16)

        # a) from x0 (same size, no upsample)
        hx0 = F.relu(self.bn0_0(self.conv0_0(x0)), inplace=True)

        # b) from x1 (2x upsample)
        hx1_ = F.interpolate(x1, scale_factor=2, mode='trilinear', align_corners=True)
        hx1 = F.relu(self.bn1_0(self.conv1_0(hx1_)), inplace=True)

        # c) from x2 (4x upsample)
        hx2_ = F.interpolate(x2, scale_factor=4, mode='trilinear', align_corners=True)
        hx2 = F.relu(self.bn2_0(self.conv2_0(hx2_)), inplace=True)

        # d) from x3 (8x upsample)
        hx3_ = F.interpolate(x3, scale_factor=8, mode='trilinear', align_corners=True)
        hx3 = F.relu(self.bn3_0(self.conv3_0(hx3_)), inplace=True)

        # e) from x4 (16x upsample)
        hx4_ = F.interpolate(x4, scale_factor=16, mode='trilinear', align_corners=True)
        hx4 = F.relu(self.bn4_0(self.conv4_0(hx4_)), inplace=True)

        # Concat
        d0 = torch.cat([hx0, hx1, hx2, hx3, hx4], dim=1)
        d0 = self.agg_conv(d0)        # -> (B, filters[0], D, H, W)

        # Final output segmentation
        out = self.out_conv(d0)       # -> (B, out_channels, D, H, W)

        return out

def compute_largest_3d_bounding_box(volumes, threshold=0):
    min_coords = np.array([np.inf, np.inf, np.inf])
    max_coords = np.array([-np.inf, -np.inf, -np.inf])

    # Iterace přes ADC a Z_ADC
    for i, volume in enumerate(volumes):
        nonzero = np.argwhere(volume > threshold)

        if nonzero.size > 0:
            min_coords = np.minimum(min_coords, nonzero.min(axis=0))
            max_coords = np.maximum(max_coords, nonzero.max(axis=0) + 1)
        else:
            print(f"Warning: Volume {i} has no nonzero voxels above threshold {threshold}. Skipping.")

    return ((int(min_coords[0]), int(max_coords[0])),
            (int(min_coords[1]), int(max_coords[1])),
            (int(min_coords[2]), int(max_coords[2])))



def crop_to_largest_bounding_box(adc_np, zadc_np, label_np, bounding_box, margin=5):
    """
    Crop all volumes (ADC, Z_ADC, Label) to the same largest 3D bounding box.

    Args:
        adc_np (np.ndarray): ADC volume.
        zadc_np (np.ndarray): Z_ADC volume.
        label_np (np.ndarray): Label volume.
        bounding_box (tuple): Bounding box ((minD, maxD), (minH, maxH), (minW, maxW)).
        margin (int): Margin to expand the bounding box.

    Returns:
        tuple: Cropped volumes (adc_cropped, zadc_cropped, label_cropped).
    """
    (minD, maxD), (minH, maxH), (minW, maxW) = bounding_box
    minD = max(0, minD - margin)
    maxD = min(adc_np.shape[0], maxD + margin)
    minH = max(0, minH - margin)
    maxH = min(adc_np.shape[1], maxH + margin)
    minW = max(0, minW - margin)
    maxW = min(adc_np.shape[2], maxW + margin)

    return (adc_np[minD:maxD, minH:maxH, minW:maxW],
            zadc_np[minD:maxD, minH:maxH, minW:maxW],
            label_np[minD:maxD, minH:maxH, minW:maxW])

def pad_3d_all_dims_to_multiple_of_32(volume_3d, mode="edge"):
    D, H, W = volume_3d.shape

    def pad_dim_to_32(dim_size):
        if dim_size % 32 == 0:
            return dim_size
        return ((dim_size - 1) // 32 + 1) * 32

    newD = pad_dim_to_32(D)
    newH = pad_dim_to_32(H)
    newW = pad_dim_to_32(W)

    padD = newD - D
    padH = newH - H
    padW = newW - W

    return np.pad(
        volume_3d,
        pad_width=((0, padD), (0, padH), (0, padW)),
        mode=mode
    )

def normalize_spacing(image_sitk, target_spacing=(1.0, 1.0, 1.0)):
    """
    Normalize image spacing to target spacing (default 1x1x1 mm)
    """
    original_spacing = image_sitk.GetSpacing()
    original_size = image_sitk.GetSize()

    # Calculate new size
    new_size = [
        int(round(osz * ospacing / tspacing))
        for osz, ospacing, tspacing in zip(original_size, original_spacing, target_spacing)
    ]

    # Perform resampling using trilinear interpolation
    resample = sitk.ResampleImageFilter()
    resample.SetOutputSpacing(target_spacing)
    resample.SetSize(new_size)
    resample.SetOutputDirection(image_sitk.GetDirection())
    resample.SetOutputOrigin(image_sitk.GetOrigin())
    resample.SetTransform(sitk.Transform())
    resample.SetDefaultPixelValue(0)

    if 'label' in str(image_sitk).lower():
        # Nearest neighbor interpolation for labels
        resample.SetInterpolator(sitk.sitkNearestNeighbor)
    else:
        # Linear interpolation for intensity images
        resample.SetInterpolator(sitk.sitkLinear)

    return resample.Execute(image_sitk)

def z_score_normalize(image_np, mask=None):
    """
    Apply z-score normalization to the image.
    If mask is provided, calculate statistics only within the mask.
    """
    if mask is not None:
        mean = np.mean(image_np[mask > 0])
        std = np.std(image_np[mask > 0])
    else:
        mean = np.mean(image_np)
        std = np.std(image_np)

    if std == 0:
        std = 1.0

    return (image_np - mean) / std

def prepare_preprocessed_data(
    adc_folder, z_folder, label_folder,
    output_adc, output_z, output_label,
    normalize=True,               # For intensity normalization
    allow_normalize_spacing=False # For spacing normalization
):
    """
    Prepare preprocessed data with optional spacing normalization and
    optional intensity normalization.
    """

    # Check if output directories have existing .mha => skip if consistent
    if not os.path.exists(output_adc):
        os.makedirs(output_adc)
    if not os.path.exists(output_z):
        os.makedirs(output_z)
    if not os.path.exists(output_label):
        os.makedirs(output_label)

    existing_adc_mhas = [f for f in os.listdir(output_adc) if f.endswith('.mha')]
    existing_z_mhas   = [f for f in os.listdir(output_z)   if f.endswith('.mha')]
    existing_label_mhas = [f for f in os.listdir(output_label) if f.endswith('.mha')]

    if (
        len(existing_adc_mhas) > 0 and
        len(existing_adc_mhas) == len(existing_z_mhas) == len(existing_label_mhas)
    ):
        print("Preprocessed data already exist. Skipping preprocessing.")
        return

    # Otherwise, proceed with preprocessing
    print("No (or incomplete) preprocessed data found. Starting preprocessing...")

    adc_files   = sorted([f for f in os.listdir(adc_folder) if f.endswith('.mha')])
    z_files     = sorted([f for f in os.listdir(z_folder)   if f.endswith('.mha')])
    label_files = sorted([f for f in os.listdir(label_folder) if f.endswith('.mha')])

    print("Starting preprocessing...")
    for idx in range(len(adc_files)):
        adc_path   = os.path.join(adc_folder,   adc_files[idx])
        zadc_path  = os.path.join(z_folder,     z_files[idx])
        label_path = os.path.join(label_folder, label_files[idx])

        adc_img   = sitk.ReadImage(adc_path)
        zadc_img  = sitk.ReadImage(zadc_path)
        label_img = sitk.ReadImage(label_path)

        # 1) Resample spacing only if allow_normalize_spacing is True
        if allow_normalize_spacing:
            adc_img   = normalize_spacing(adc_img)
            zadc_img  = normalize_spacing(zadc_img)
            label_img = normalize_spacing(label_img)

        # Convert to numpy
        adc_np   = sitk.GetArrayFromImage(adc_img)
        zadc_np  = sitk.GetArrayFromImage(zadc_img)
        label_np = sitk.GetArrayFromImage(label_img)

        # 2) Intensity normalization (z-score) only if normalize=True
        if normalize:
            brain_mask = (adc_np > 0) & (zadc_np > 0)
            adc_np  = z_score_normalize(adc_np,  brain_mask)
            zadc_np = z_score_normalize(zadc_np, brain_mask)

        # 3) Crop bounding box
        bounding_box = compute_largest_3d_bounding_box([adc_np, zadc_np], threshold=0)
        adc_np, zadc_np, label_np = crop_to_largest_bounding_box(
            adc_np, zadc_np, label_np, bounding_box, margin=5
        )

        # 4) Pad to multiple of 32
        adc_np   = pad_3d_all_dims_to_multiple_of_32(adc_np)
        zadc_np  = pad_3d_all_dims_to_multiple_of_32(zadc_np)
        label_np = pad_3d_all_dims_to_multiple_of_32(label_np)

        # 5) Convert back to SimpleITK and set metadata
        adc_preprocessed   = sitk.GetImageFromArray(adc_np)
        zadc_preprocessed  = sitk.GetImageFromArray(zadc_np)
        label_preprocessed = sitk.GetImageFromArray(label_np)

        # Use uniform spacing if allow_normalize_spacing is True, else original
        if allow_normalize_spacing:
            spacing = (1.0, 1.0, 1.0)
        else:
            spacing = adc_img.GetSpacing()

        for img in [adc_preprocessed, zadc_preprocessed, label_preprocessed]:
            img.SetSpacing(spacing)
            img.SetDirection(adc_img.GetDirection())
            img.SetOrigin(adc_img.GetOrigin())

        # 6) Save to Drive
        sitk.WriteImage(adc_preprocessed,  os.path.join(output_adc,   adc_files[idx]))
        sitk.WriteImage(zadc_preprocessed, os.path.join(output_z,     z_files[idx]))
        sitk.WriteImage(label_preprocessed,os.path.join(output_label, label_files[idx]))

    print("Preprocessing completed! Data are saved permanently.")

DEBUG_FILTER = False  # Nastavte na True pro debug výpisy

def filter_augmented_files(file_list, max_aug):
    grouped = {}
    for f in file_list:
        key = get_base_id(f)  # Použijeme novou funkci pro konzistentní klíč
        if '_orig_' in f.lower():
            if key not in grouped:
                grouped[key] = {'orig': None, 'aug': []}
            grouped[key]['orig'] = f
        elif '_aug' in f.lower():
            if key not in grouped:
                grouped[key] = {'orig': None, 'aug': []}
            grouped[key]['aug'].append(f)
        else:
            if key not in grouped:
                grouped[key] = {'orig': f, 'aug': []}
    if DEBUG_FILTER:
        print("=== Debug: Skupiny před filtrováním ===")
        for key, entry in grouped.items():
            print(f"Key: {key} | Orig: {entry['orig']} | Aug: {entry['aug']}")

    filtered_list = []
    for key in sorted(grouped.keys()):
        entry = grouped[key]
        if entry['orig'] is not None:
            filtered_list.append(entry['orig'])
            selected_aug = sorted(entry['aug'])[:max_aug]
            filtered_list.extend(selected_aug)
            if DEBUG_FILTER:
                print(f"Key: {key} -> Ponechán originál: {entry['orig']} a {len(selected_aug)} _aug soubor(y): {selected_aug}")
        else:
            selected = sorted(entry['aug'])[:max_aug]
            filtered_list.extend(selected)
            if DEBUG_FILTER:
                print(f"Key: {key} -> Originál chybí, ponecháno pouze {len(selected)} _aug soubor(y): {selected}")
    if DEBUG_FILTER:
        print(f"Celkový počet souborů původně: {len(file_list)}, po filtrování: {len(filtered_list)}")
    return filtered_list

###############################################################################
# 2) Data Augmentation: Random Rotation and Horizontal Flip                   #
###############################################################################
from scipy.ndimage import rotate, gaussian_filter

def random_3d_augmentation(
    adc_np,
    zadc_np,
    label_np,
    angle_max=3,
    p_flip=0.5,
    p_noise=0.0,
    noise_std=0.01,
    p_smooth=0.0,
    smooth_sigma=1.0
):
    if loss_name == "focal":
        angle_max = 2
        p_noise = 0.0
        smooth_sigma = 0.2
        p_smooth = 0.00

    if random.random() < p_flip:
      adc_np = np.flip(adc_np, axis=2).copy()
      zadc_np = np.flip(zadc_np, axis=2).copy()
      label_np = np.flip(label_np, axis=2).copy()

    # 2) Random rotation
    angle = random.uniform(-angle_max, angle_max)
    axes  = random.choice([(0, 1), (0, 2), (1, 2)])  # vybereme jednu dvojici os
    adc_np = rotate(adc_np,   angle=angle, axes=axes, reshape=False, order=1, mode='nearest')
    zadc_np= rotate(zadc_np,  angle=angle, axes=axes, reshape=False, order=1, mode='nearest')
    label_np = rotate(label_np, angle=angle, axes=axes, reshape=False, order=0, mode='nearest')

    # 3) Gaussian noise
    if random.random() < p_noise:
        noise_adc = np.random.normal(0, noise_std, size=adc_np.shape)
        noise_z   = np.random.normal(0, noise_std, size=zadc_np.shape)
        adc_np   = adc_np + noise_adc
        zadc_np  = zadc_np + noise_z

    # 4) Gaussian smoothing
    if random.random() < p_smooth:
        adc_np   = gaussian_filter(adc_np, sigma=smooth_sigma)
        zadc_np  = gaussian_filter(zadc_np, sigma=smooth_sigma)

    return adc_np, zadc_np, label_np

###############################################################################
# 3) Dataset class for TRAINING with augmentation                #
###############################################################################
class BONBID3DFullVolumeDataset(Dataset):
    """
    Čte 3D volume (ADC, Z_ADC, LABEL) z daných složek. Lze omezit dataset pouze
    na subjekty s patient ID z allowed_patient_ids.
    """
    def __init__(self, adc_folder, z_folder, label_folder, augment=False, allowed_patient_ids=None):
        super().__init__()

        self.adc_folder = adc_folder
        self.z_folder = z_folder
        self.label_folder = label_folder
        self.augment = augment

        self.adc_files = sorted([f for f in os.listdir(adc_folder) if f.endswith('.mha')])
        self.z_files   = sorted([f for f in os.listdir(z_folder) if f.endswith('.mha')])
        self.lab_files = sorted([f for f in os.listdir(label_folder) if f.endswith('.mha')])

        if extended_dataset:
            self.adc_files = filter_augmented_files(self.adc_files, max_aug_per_orig)
            self.z_files   = filter_augmented_files(self.z_files, max_aug_per_orig)
            self.lab_files = filter_augmented_files(self.lab_files, max_aug_per_orig)
            print(f"DEBUG DATASET: Po filtrování -> ADC: {len(self.adc_files)} souborů, Z_ADC: {len(self.z_files)} souborů, LABEL: {len(self.lab_files)} souborů")

        if allowed_patient_ids is not None:
            self.adc_files = [f for f in self.adc_files if get_patient_numeric_id(f) in allowed_patient_ids]
            self.z_files   = [f for f in self.z_files if get_patient_numeric_id(f) in allowed_patient_ids]
            self.lab_files = [f for f in self.lab_files if get_patient_numeric_id(f) in allowed_patient_ids]

        if not (len(self.adc_files) == len(self.z_files) == len(self.lab_files)):
            raise ValueError("Mismatch in .mha file counts among ADC, Z_ADC, LABEL.")


    def __len__(self):
        return len(self.adc_files)

    def __getitem__(self, idx):
        adc_path   = os.path.join(self.adc_folder, self.adc_files[idx])
        zadc_path  = os.path.join(self.z_folder,   self.z_files[idx])
        label_path = os.path.join(self.label_folder, self.lab_files[idx])

        adc_np   = sitk.GetArrayFromImage(sitk.ReadImage(adc_path))
        zadc_np  = sitk.GetArrayFromImage(sitk.ReadImage(zadc_path))
        label_np = sitk.GetArrayFromImage(sitk.ReadImage(label_path))

        if self.augment:
            adc_np, zadc_np, label_np = random_3d_augmentation(adc_np, zadc_np, label_np)

        # Převedeme na Torch tensor – vstupní tvar bude (2, D, H, W)
        adc_t  = torch.from_numpy(adc_np).unsqueeze(0)
        zadc_t = torch.from_numpy(zadc_np).unsqueeze(0)
        input_3d = torch.cat([adc_t, zadc_t], dim=0)
        label_t  = torch.from_numpy(label_np).long()
        return input_3d.float(), label_t




class IndexedDatasetWrapper(Dataset):
    def __init__(self, dataset):
        self.dataset = dataset

    def __getitem__(self, index):
        data, label = self.dataset[index]
        return data, label, index

    def __len__(self):
        return len(self.dataset)


class BONBID3DPatchDataset(Dataset):
    """
    Třída, která obalí full-volume dataset a při každém volání __getitem__
    náhodně vybere patch z daného objemu.

    Args:
        full_volume_dataset: Instance již existujícího full-volume datasetu (např. BONBID3DFullVolumeDataset nebo jeho Subset).
        patch_size (tuple): Velikost patche, např. (64,64,64).
        patches_per_volume (int): Kolik patchí se bude extrahovat z každého objemu.
        augment (bool): Použít nebo ne data augmentaci na jednotlivých patchech.
    """
    def __init__(self, full_volume_dataset, patch_size=(64,64,64), patches_per_volume=10, augment=False):
        super().__init__()
        self.full_volume_dataset = full_volume_dataset
        self.patch_size = patch_size
        self.patches_per_volume = patches_per_volume
        self.augment = augment
        self.num_volumes = len(full_volume_dataset)

    def __len__(self):
        return self.num_volumes * self.patches_per_volume

    def __getitem__(self, idx):
        # Určíme, z kterého objemu bude patch
        volume_idx = idx // self.patches_per_volume
        input_vol, label_vol = self.full_volume_dataset[volume_idx]  # input_vol: (C, D, H, W), label_vol: (D, H, W)

        # Převedeme na numpy (pro snadné ořezy)
        input_np = input_vol.numpy()  # tvar (C, D, H, W)
        label_np = label_vol.numpy()  # tvar (D, H, W)

        _, D, H, W = input_np.shape
        pD, pH, pW = self.patch_size

        # Určíme náhodný počáteční bod (kontrolujeme, aby patch pasoval)
        d0 = random.randint(0, max(0, D - pD))
        h0 = random.randint(0, max(0, H - pH))
        w0 = random.randint(0, max(0, W - pW))

        # Ořízneme patch z každého kanálu a labelu
        patch_input = input_np[:, d0:d0+pD, h0:h0+pH, w0:w0+pW]
        patch_label = label_np[d0:d0+pD, h0:h0+pH, w0:w0+pW]

        # Pokud chcete augmentaci na patchi, použijte ji zde.
        if self.augment:
            # Uvažujeme, že první kanál je ADC a druhý je Z_ADC – zavoláme tedy existující funkci
            adc_patch = patch_input[0]
            zadc_patch = patch_input[1]
            adc_patch, zadc_patch, patch_label = random_3d_augmentation(adc_patch, zadc_patch, patch_label)
            patch_input = np.stack([adc_patch, zadc_patch], axis=0)

        # Převedeme zpět na Torch tensory
        patch_input = torch.from_numpy(patch_input).float()
        patch_label = torch.from_numpy(patch_label).long()

        return patch_input, patch_label

###############################################################################
# 5) Loss functions                                                           #
###############################################################################

def get_loss_function(loss_name, alpha, class_weights=None,
                      ft_alpha=0.7, ft_beta=0.3, ft_gamma=4/3):

    if loss_name == "weighted_ce_dice":
        ce_criterion = torch.nn.CrossEntropyLoss(weight=class_weights)
        def loss_fn(logits, labels):
            return weighted_ce_plus_dice_loss(
                logits=logits,
                labels=labels,
                ce_criterion=ce_criterion,
                alpha=alpha
            )
        return loss_fn

    elif loss_name == "log_cosh_dice":
        def loss_fn(logits, labels):
            return log_cosh_dice_loss(logits, labels)
        return loss_fn

    elif loss_name == "focal_ce_combo":
        ce_criterion = nn.CrossEntropyLoss(weight=class_weights)

        def loss_fn(logits, labels):
            focal_val = focal_loss(logits, labels, alpha=focal_alpha, gamma=focal_gamma)
            ce_val    = ce_criterion(logits, labels)
            return 0.8 * focal_val + 0.25 * ce_val

        return loss_fn

    elif loss_name == "focal_dice_combo":
        def loss_fn(logits, labels):
            return combined_focal_dice_loss(
                logits=logits,
                labels=labels,
                focal_alpha=focal_alpha,
                focal_gamma=focal_gamma,
                alpha_mix=alpha_mix
            )
        return loss_fn

    elif loss_name == "focal_tversky":
        # Pevné parametry => alpha=0.6, beta=0.4, gamma=1.3
        def loss_fn(logits, labels):
            return focal_tversky_loss(
                logits=logits,
                labels=labels,
                alpha=0.3,
                beta=0.7,
                gamma=2
            )
        return loss_fn

    elif loss_name == "dice_focal":
      dice_focal_loss = DiceFocalLoss(
          include_background=True,
          to_onehot_y=False,
          softmax=True,
          squared_pred=True,
          alpha=focal_alpha,
          gamma=focal_gamma
      )

      def loss_fn(logits, labels):
          labels = one_hot(labels.unsqueeze(1), num_classes=out_channels)
          return dice_focal_loss(logits, labels)

      return loss_fn

    elif loss_name == "focal":
        def loss_fn(logits, labels):
            return focal_loss(
                logits,
                labels,
                alpha=0.8,
                gamma=2.0
            )
        return loss_fn

    elif loss_name == "log_hausdorff":
        def loss_fn(logits, labels):
            return log_hausdorff_loss(logits, labels, alpha=2.0)
        return loss_fn

    else:
        raise ValueError(f"Unsupported loss_name: {loss_name}")

from scipy.ndimage import distance_transform_edt

def combined_focal_dice_loss(
    logits: torch.Tensor,
    labels: torch.Tensor,
    focal_alpha: float = 0.8,
    focal_gamma: float = 2.0,
    alpha_mix:  float = 0.8,
    eps:        float = 1e-6
) -> torch.Tensor:
    focal_val = focal_loss(
        logits=logits,
        labels=labels,
        alpha=focal_alpha,
        gamma=focal_gamma,
        eps=eps
    )

    dice_val = soft_dice_loss(logits, labels, smooth=1.0)

    combined = alpha_mix * focal_val + (1.0 - alpha_mix) * dice_val
    return combined

def focal_loss(
    logits: torch.Tensor,
    labels: torch.Tensor,
    alpha: float = 0.8,
    gamma: float = 2.0,
    eps:   float = 1e-6
) -> torch.Tensor:


    probs = F.softmax(logits, dim=1)[:, 1, ...]  # shape: [B, D, H, W]

    fg_mask = (labels == 1).float()

    probs = probs.clamp(min=eps, max=1.0 - eps)

    focal_fg = - alpha * fg_mask * ((1.0 - probs) ** gamma) * torch.log(probs)
    focal_bg = - (1.0 - alpha) * (1.0 - fg_mask) * (probs ** gamma) * torch.log(1.0 - probs)

    focal_loss_val = (focal_fg + focal_bg).mean()
    return focal_loss_val

def log_hausdorff_loss(
    logits: torch.Tensor,
    labels: torch.Tensor,
    alpha: float = 2.0,
    eps:   float = 1e-5
) -> torch.Tensor:

    prob = F.softmax(logits, dim=1)[:, 1, ...]   # tvar [B, D, H, W]
    B = prob.shape[0]

    loss_accum = 0.0
    for i in range(B):
        p_mask_np = (prob[i] >= 0.5).detach().cpu().numpy().astype(np.uint8)
        q_mask_np = labels[i].detach().cpu().numpy().astype(np.uint8)

        dp = distance_transform_edt(1 - p_mask_np)
        dq = distance_transform_edt(1 - q_mask_np)

        diff = (p_mask_np - q_mask_np)**2

        term_map = diff * ((dp**alpha) + (dq**alpha))
        mean_term = term_map.mean()

        loss_i = np.log(1.0 + mean_term + eps)
        loss_accum += loss_i

    loss_value = loss_accum / B
    return torch.tensor(loss_value, dtype=torch.float, requires_grad=True).to(logits.device)

def focal_tversky_loss(
    logits: torch.Tensor,
    labels: torch.Tensor,
    alpha: float = 0.7,
    beta: float  = 0.3,
    gamma: float = 4/3,
    smooth: float= 1.0
) -> torch.Tensor:
    probs = F.softmax(logits, dim=1)
    fg_prob = probs[:, 1, ...]

    fg_label = (labels == 1).float()

    dims = (1,2,3)
    tp = (fg_prob * fg_label).sum(dim=dims)
    fp = ((1.0 - fg_label) * fg_prob).sum(dim=dims)
    fn = (fg_label * (1.0 - fg_prob)).sum(dim=dims)

    tversky_index = (tp + smooth) / (tp + alpha*fn + beta*fp + smooth)

    ftl = (1.0 - tversky_index) ** gamma

    return ftl.mean()


def soft_dice_loss(logits, labels, smooth=1.0):
    prob = F.softmax(logits, dim=1)
    fg   = prob[:,1]
    target_fg = (labels == 1).float()
    inter = (fg * target_fg).sum()
    union= fg.sum() + target_fg.sum()
    dice = (2.*inter + smooth)/(union + smooth)
    return 1.0 - dice

def dice_coefficient(pred, labels, smooth=1.0):
    pred_fg = (pred == 1).astype(np.float32)
    target_fg = (labels == 1).astype(np.float32)

    inter = (pred_fg * target_fg).sum()
    union = pred_fg.sum() + target_fg.sum()
    dice = (2. * inter + smooth) / (union + smooth)

    return dice

def weighted_ce_plus_dice_loss(logits, labels, ce_criterion, alpha=0.5):
    """
    Weighted Cross Entropy + alpha * Soft Dice Loss
    """
    ce_loss = ce_criterion(logits, labels)     # Weighted CE
    dice_l  = soft_dice_loss(logits, labels)   # 1 - Dice coefficient
    return ce_loss + alpha * dice_l

def log_cosh_dice_loss(logits, labels, smooth=1.0):
    """
    Compute the Log-Cosh Dice Loss.

    Args:
        logits (torch.Tensor): Shape (B, 2, D, H, W). Raw model outputs.
        labels (torch.Tensor): Shape (B, D, H, W). Values 0 or 1.
        smooth (float): Smoothing constant to avoid division by zero.

    Returns:
        Scalar tensor representing the Log-Cosh Dice Loss.
    """
    # Probabilities via softmax, pick foreground channel
    prob = F.softmax(logits, dim=1)  # [B, 2, D, H, W]
    fg   = prob[:, 1, ...]          # foreground probability map
    target_fg = (labels == 1).float()

    intersection = (fg * target_fg).sum(dim=[1,2,3])   # sum per batch item
    union        = fg.sum(dim=[1,2,3]) + target_fg.sum(dim=[1,2,3])

    dice = (2.0 * intersection + smooth) / (union + smooth)  # per-batch dice
    dice_loss = 1.0 - dice                                   # [B]-shaped

    # Apply log-cosh
    log_cosh = torch.log(torch.cosh(dice_loss))
    # Average across the batch dimension
    return log_cosh.mean()

def compute_surface_distances(pred_mask, gt_mask, spacing=(1.0, 1.0, 1.0), sampling_ratio=0.5):
    """
    Optimalizovaný výpočet povrchových vzdáleností.

    Args:
        pred_mask: Predikovaná binární maska
        gt_mask: Ground truth binární maska
        spacing: Voxel spacing (default: 1mm isotropic)
        sampling_ratio: Poměr bodů použitých pro výpočet (0-1), pro urychlení
    """

    # Získání povrchových voxelů pomocí XOR s erozí
    pred_surface = np.logical_xor(pred_mask, binary_erosion(pred_mask))
    gt_surface = np.logical_xor(gt_mask, binary_erosion(gt_mask))

    # Výpočet vzdálenostní mapy
    gt_distance = distance_transform_edt(~gt_surface, sampling=spacing)

    # Volitelné podvzorkování povrchových bodů pro urychlení
    if sampling_ratio < 1.0:
        pred_surface_points = np.argwhere(pred_surface)
        num_points = len(pred_surface_points)
        num_sampled = int(num_points * sampling_ratio)
        if num_sampled > 0:
            indices = np.random.choice(num_points, num_sampled, replace=False)
            surface_distances = gt_distance[tuple(pred_surface_points[indices].T)]
        else:
            surface_distances = gt_distance[pred_surface]
    else:
        surface_distances = gt_distance[pred_surface]

    return surface_distances

def compute_masd(pred_mask, gt_mask, spacing=(1.0, 1.0, 1.0), sampling_ratio=0.5):
    if not np.any(pred_mask) and not np.any(gt_mask):
        return 0.0

    distances_pred_to_gt = compute_surface_distances(pred_mask, gt_mask, spacing, sampling_ratio)
    distances_gt_to_pred = compute_surface_distances(gt_mask, pred_mask, spacing, sampling_ratio)

    if len(distances_pred_to_gt) == 0 and len(distances_gt_to_pred) == 0:
        return 0.0
    elif len(distances_pred_to_gt) == 0:
        return np.mean(distances_gt_to_pred)
    elif len(distances_gt_to_pred) == 0:
        return np.mean(distances_pred_to_gt)

    mean_distance = (np.mean(distances_pred_to_gt) + np.mean(distances_gt_to_pred)) / 2.0
    return mean_distance

def compute_nsd(pred_mask, gt_mask, spacing=(1.0, 1.0, 1.0), tau=1.0, sampling_ratio=0.5):
    """
    Optimalizovaný výpočet Normalized Surface Dice (NSD)
    """
    # Rychlá kontrola prázdných masek
    if not np.any(pred_mask) and not np.any(gt_mask):
        return 1.0
    if not np.any(pred_mask) or not np.any(gt_mask):
        return 0.0

    # Výpočet vzdáleností v obou směrech s podvzorkováním
    distances_pred_to_gt = compute_surface_distances(
        pred_mask, gt_mask, spacing, sampling_ratio)
    distances_gt_to_pred = compute_surface_distances(
        gt_mask, pred_mask, spacing, sampling_ratio)

    # Počet bodů v rámci tolerance
    pred_within_tau = np.sum(distances_pred_to_gt <= tau)
    gt_within_tau = np.sum(distances_gt_to_pred <= tau)

    # Celkový počet povrchových bodů (škálovaný podle sampling_ratio)
    total_pred_surface = len(distances_pred_to_gt) / sampling_ratio
    total_gt_surface = len(distances_gt_to_pred) / sampling_ratio

    # Výpočet NSD
    nsd = (pred_within_tau + gt_within_tau) / (total_pred_surface + total_gt_surface)
    return nsd

def create_scheduler_cosine(optimizer, T_max, eta_min=1e-5):
    """
    Vytvoří cosinový scheduler (od LR = initial až do LR=eta_min).
    T_max = počet epoch (nebo kroků) pro úplný "cyklus".
    """
    return lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)


def compute_loss_per_sample(loss_fn, logits, labels):
    """
    Vypočítá ztrátu pro každý vzorek zvlášť.
    Předpokládáme, že logits má tvar [B, ...] a labels [B, ...].
    """
    batch_losses = []
    for i in range(logits.shape[0]):
        # Vybereme i-tý vzorek a přidáme dimenzi batch
        sample_loss = loss_fn(logits[i].unsqueeze(0), labels[i].unsqueeze(0))
        batch_losses.append(sample_loss)
    return torch.stack(batch_losses)  # vrátí tensor tvaru [B]

###############################################################################
# 6) Train / Validate for patch-based approach                               #
###############################################################################
def train_one_epoch(model, loader, optimizer, loss_fn, device="cuda"):
    model.train()
    running_loss = 0.0

    for inputs, labels in loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        logits = model(inputs)
        loss = loss_fn(logits, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    return running_loss / len(loader)

def validate_one_epoch(model, loader, loss_fn, device="cuda"):
    model.eval()
    running_loss = 0.0
    running_dice = 0.0
    running_masd = 0.0
    running_nsd  = 0.0
    count_samples = 0

    tta_transforms = get_tta_transforms(angle_max=TTA_ANGLE_MAX) if USE_TTA else None

    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)

            if training_mode == "patch":
                logits = sliding_window_inference(inputs, patch_size, batch_size, model, overlap=0.25)
            else:
                logits = model(inputs)

            loss = loss_fn(logits, labels)
            running_loss += loss.item()

            if USE_TTA:
                batch_preds = []
                for i in range(inputs.shape[0]):
                    input_i = inputs[i].unsqueeze(0)
                    avg_probs = tta_forward(model, input_i, device, tta_transforms)
                    pred_i = np.argmax(avg_probs, axis=0)
                    batch_preds.append(pred_i)
                preds = np.array(batch_preds)
            else:
                preds = torch.argmax(logits, dim=1).cpu().numpy()

            labs = labels.cpu().numpy()
            for pred, label in zip(preds, labs):
                dsc = dice_coefficient(pred, label)
                running_dice += dsc

                if compute_surface_metrics:
                    masd = compute_masd(pred, label, spacing=(1,1,1), sampling_ratio=0.5)
                    nsd  = compute_nsd(pred, label, spacing=(1,1,1), sampling_ratio=0.5)
                    running_masd += masd
                    running_nsd  += nsd
                count_samples += 1

    avg_loss = running_loss / len(loader)
    avg_dice = running_dice / count_samples if count_samples > 0 else 0.0

    metrics = {'val_loss': avg_loss, 'val_dice': avg_dice}
    if compute_surface_metrics and count_samples > 0:
        metrics['val_masd'] = running_masd / count_samples
        metrics['val_nsd']  = running_nsd  / count_samples

    return metrics

def run_cross_validation_full_volume(
    adc_folder,
    z_folder,
    label_folder,
    n_folds,
    lr,
    eta_min,
    use_augmentation=True,
    device="cuda",
    epochs=50,
    batch_size=1,
):
    """
    Subject-based cross-validation.

    Pokud extended_dataset == True:
      1) Ze souborů se vybírají subjekty podle názvu.
      2) Pro validaci se použijí pouze _orig_ soubory (pouze originály) u subjektů v daném foldu.
      3) Pro trénink se použijí všechny soubory (originály + augmentované) u subjektů, které nejsou ve validaci.
      4) Subjekty, které se objeví ve validační sadě, se v tréninku vůbec nepoužijí.

    Pokud extended_dataset == False:
      Dataset obsahuje pouze originály – tedy žádná rozlišení _orig_ / _aug_. Všechny soubory jsou klasické.
      Rozdělení se pak provádí standardně, bez filtrování názvů.
    """

    # Vytvoříme dvě instance datasetu – jedna bez augmentace (pro validaci)
    # a druhá s augmentací (pro trénink)
    full_dataset_no_aug = BONBID3DFullVolumeDataset(
        adc_folder=adc_folder,
        z_folder=z_folder,
        label_folder=label_folder,
        augment=False
    )
    full_dataset_aug = BONBID3DFullVolumeDataset(
        adc_folder=adc_folder,
        z_folder=z_folder,
        label_folder=label_folder,
        augment=use_augmentation
    )


    adc_files_full = full_dataset_no_aug.adc_files
    z_files_full   = full_dataset_no_aug.z_files
    lab_files_full = full_dataset_no_aug.lab_files

    subject_to_indices = {}
    for i, fname in enumerate(adc_files_full):
        subj_id = get_subject_id_from_filename(fname)
        if subj_id not in subject_to_indices:
            subject_to_indices[subj_id] = []
        subject_to_indices[subj_id].append(i)

    all_subjects = list(subject_to_indices.keys())
    random.shuffle(all_subjects)

    num_subjects = len(all_subjects)
    fold_size = num_subjects // n_folds

    all_val_losses = []
    all_val_dices  = []

    print(f"Total {num_subjects} unique subjects, performing {n_folds}-fold subject-wise CV.\n")

    for fold_idx in range(n_folds):
        print(f"=== FOLD {fold_idx+1}/{n_folds} ===")

        fold_val_start = fold_idx * fold_size
        if fold_idx < n_folds - 1:
            fold_val_end = (fold_idx + 1) * fold_size
        else:
            fold_val_end = num_subjects

        val_subjects = all_subjects[fold_val_start:fold_val_end]
        train_subjects = list(set(all_subjects) - set(val_subjects))

        # 2) Výběr indexů pro validaci
        val_indices = []
        if extended_dataset:
            # Pokud máme rozšířený dataset, vybíráme pouze _orig_ soubory
            for subj_id in val_subjects:
                indices_for_subject = subject_to_indices[subj_id]
                for idx in indices_for_subject:
                    adc_fname = adc_files_full[idx]
                    if "_orig_" in adc_fname.lower():
                        val_indices.append(idx)
        else:
            # Klasický dataset: použijeme všechny soubory daného subjektu
            for subj_id in val_subjects:
                indices_for_subject = subject_to_indices[subj_id]
                val_indices.extend(indices_for_subject)

        # 3) Výběr indexů pro trénink – pro trénink se vždy použijí všechny soubory subjektů, které nejsou ve validaci
        train_indices = []
        for subj_id in train_subjects:
            indices_for_subject = subject_to_indices[subj_id]
            train_indices.extend(indices_for_subject)

        print(f"  Fold {fold_idx+1}: #Val subjects={len(val_subjects)}, #Val indices={len(val_indices)}")
        print(f"                #Train subjects={len(train_subjects)}, #Train indices={len(train_indices)}")

        # Vytvoříme subsety datasetu
        train_ds = Subset(full_dataset_aug, train_indices)
        val_ds   = Subset(full_dataset_no_aug, val_indices)

        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
        val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)

        # Vytvoření modelu, loss funkce, optimizeru a scheduleru
        model = create_model(model_name, in_channels, out_channels).to(device)

        class_weights = torch.tensor([bg_weight, fg_weight], device=device, dtype=torch.float)

        loss_fn = get_loss_function(
            loss_name=loss_name,
            alpha=alpha,
            class_weights=class_weights if loss_name == "weighted_ce_dice" else None
        )

        optimizer = optim.Adam(model.parameters(), lr=lr)
        scheduler = lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=epochs,
            eta_min=eta_min
        )

        fold_train_losses = []
        fold_val_losses   = []
        fold_val_dices    = []

        for epoch in range(1, epochs + 1):
            if use_ohem:
                model.eval()
                dice_list = []
                train_loader_no_shuffle = DataLoader(train_ds, batch_size=1, shuffle=False)

                # Získat Dice skóre pro všechny trénovací vzorky
                with torch.no_grad():
                    for i, (inputs, labels) in enumerate(train_loader_no_shuffle):
                        inputs, labels = inputs.to(device), labels.to(device)
                        logits = model(inputs)
                        pred = torch.argmax(logits, dim=1)
                        pred_np = pred.cpu().numpy()[0]
                        labels_np = labels.cpu().numpy()[0]
                        dice = dice_coefficient(pred_np, labels_np)
                        dice_list.append((dice, i))

                # Vypočítat váhy pro každý vzorek
                weights = [1.0 - dice for dice, _ in dice_list]

                # Normalizovat váhy na rozsah [0.5, 1.5]
                min_w = min(weights)
                max_w = max(weights)
                if max_w != min_w:  # Zamezení dělení nulou
                    normalized_weights = [(w - min_w)/(max_w - min_w) + 0.5 for w in weights]
                else:
                    normalized_weights = [1.0]*len(weights)

                # Vytvořit vážený sampler
                sampler = WeightedRandomSampler(
                    weights=normalized_weights,
                    num_samples=len(train_ds),
                    replacement=True
                )

                # Vytvořit nový DataLoader se samplerem
                weighted_train_loader = DataLoader(
                    train_ds,
                    batch_size=batch_size,
                    sampler=sampler,
                    drop_last=True
                )

                print(f"Epoch {epoch}: Using weighted sampling (OHEM)")
                train_loss = train_one_epoch(model, weighted_train_loader, optimizer, loss_fn, device=device)
            else:
                train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device=device)

            val_metrics = validate_one_epoch(model, val_loader, loss_fn, device=device)
            scheduler.step()

            fold_train_losses.append(train_loss)
            fold_val_losses.append(val_metrics['val_loss'])
            fold_val_dices.append(val_metrics['val_dice'])

            current_lr = scheduler.get_last_lr()[0]
            log_str = (f"[FOLD {fold_idx+1}] Epoch {epoch}/{epochs} => "
                       f"Train loss={train_loss:.4f}, Val loss={val_metrics['val_loss']:.4f}, "
                       f"Val Dice={val_metrics['val_dice']:.4f}, lr={current_lr:.6f}")
            if compute_surface_metrics:
                log_str += (f", val_masd={val_metrics.get('val_masd', 0):.4f}, "
                            f"val_nsd={val_metrics.get('val_nsd', 0):.4f}")
            print(log_str)

            wandb_log = {
                'fold': fold_idx + 1,
                'epoch': epoch,
                'train_loss': train_loss,
                'val_loss': val_metrics['val_loss'],
                'val_dice': val_metrics['val_dice'],
                'lr': current_lr
            }
            if compute_surface_metrics:
                wandb_log.update({
                    'val_masd': val_metrics.get('val_masd', 0),
                    'val_nsd':  val_metrics.get('val_nsd', 0),
                })
            wandb.log(wandb_log)

            #if epoch == 10:
                #val_results = infer_full_volume_on_val_set_full(
                    #model=model,
                    #val_indices=val_indices,
                    #adc_files=adc_files_full,
                    #z_files=z_files_full,
                    #label_files=lab_files_full,
                    #adc_folder=adc_folder,
                    #z_folder=z_folder,
                    #label_folder=label_folder,
                    #device=device
                #)

        # --- Full-volume inference na validační sadě ---
        val_results = infer_full_volume_on_val_set_full(
            model=model,
            val_indices=val_indices,
            adc_files=adc_files_full,
            z_files=z_files_full,
            label_files=lab_files_full,
            adc_folder=adc_folder,
            z_folder=z_folder,
            label_folder=label_folder,
            device=device
        )

        all_val_losses.append(fold_val_losses[-1])
        all_val_dices.append(fold_val_dices[-1])

    # Finální shrnutí
    mean_val_loss = np.mean(all_val_losses)
    mean_val_dice = np.mean(all_val_dices)
    print("\n=== SUBJECT-WISE FULLVOL CROSS-VALIDATION RESULTS ===")
    for i in range(n_folds):
        print(f" Fold {i+1} => val_loss={all_val_losses[i]:.4f}, val_dice={all_val_dices[i]:.4f}")
    print(f" Overall => mean_val_loss={mean_val_loss:.4f}, mean_val_dice={mean_val_dice:.4f}")

    return mean_val_loss, mean_val_dice


def run_cross_validation_patch_based(
    adc_folder,
    z_folder,
    label_folder,
    n_folds,
    lr,
    eta_min,
    use_augmentation=True,
    device="cuda",
    epochs=50,
    batch_size=1,
    patches_per_volume=10,
    patch_size=(64,64,64)
):
    # Vytvoříme full-volume datasety (pro validaci i pro získávání patchí)
    full_dataset_no_aug = BONBID3DFullVolumeDataset(
        adc_folder=adc_folder,
        z_folder=z_folder,
        label_folder=label_folder,
        augment=False
    )
    full_dataset_aug = BONBID3DFullVolumeDataset(
        adc_folder=adc_folder,
        z_folder=z_folder,
        label_folder=label_folder,
        augment=False  # augmentace bude provedena při extrakci patchí, pokud je nastaveno
    )

    # Rozdělení podle subjektů (stejně jako ve full-volume variantě)
    adc_files_full = full_dataset_no_aug.adc_files
    z_files_full   = full_dataset_no_aug.z_files
    lab_files_full = full_dataset_no_aug.lab_files

    subject_to_indices = {}
    for i, fname in enumerate(adc_files_full):
        subj_id = get_subject_id_from_filename(fname)
        if subj_id not in subject_to_indices:
            subject_to_indices[subj_id] = []
        subject_to_indices[subj_id].append(i)

    all_subjects = list(subject_to_indices.keys())
    random.shuffle(all_subjects)
    num_subjects = len(all_subjects)
    fold_size = num_subjects // n_folds

    all_val_losses = []
    all_val_dices  = []

    print(f"Total {num_subjects} unique subjects, performing {n_folds}-fold subject-wise CV for patch-based training.\n")

    for fold_idx in range(n_folds):
        print(f"=== FOLD {fold_idx+1}/{n_folds} ===")
        fold_val_start = fold_idx * fold_size
        fold_val_end = (fold_idx + 1) * fold_size if fold_idx < n_folds - 1 else num_subjects
        val_subjects = all_subjects[fold_val_start:fold_val_end]
        train_subjects = list(set(all_subjects) - set(val_subjects))

        # Vybereme indexy pro validaci
        val_indices = []
        if extended_dataset:
            for subj_id in val_subjects:
                indices_for_subject = subject_to_indices[subj_id]
                for idx in indices_for_subject:
                    adc_fname = adc_files_full[idx]
                    if "_orig_" in adc_fname.lower():
                        val_indices.append(idx)
        else:
            for subj_id in val_subjects:
                indices_for_subject = subject_to_indices[subj_id]
                val_indices.extend(indices_for_subject)

        # Vybereme indexy pro trénink
        train_indices = []
        for subj_id in train_subjects:
            indices_for_subject = subject_to_indices[subj_id]
            train_indices.extend(indices_for_subject)

        print(f"  Fold {fold_idx+1}: #Val subjects={len(val_subjects)}, #Val indices={len(val_indices)}")
        print(f"                #Train subjects={len(train_subjects)}, #Train indices={len(train_indices)}")

        # Vytvoříme subsety full-volume datasetu
        train_full_ds = Subset(full_dataset_aug, train_indices)
        val_ds = Subset(full_dataset_no_aug, val_indices)

        # Pro trénink použijeme patch dataset
        train_patch_ds = BONBID3DPatchDataset(
            full_volume_dataset=train_full_ds,
            patch_size=patch_size,
            patches_per_volume=patches_per_volume,
            augment=use_augmentation
        )

        train_loader = DataLoader(train_patch_ds, batch_size=batch_size, shuffle=True)
        val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)

        model = create_model(model_name, in_channels, out_channels).to(device)

        class_weights = torch.tensor([bg_weight, fg_weight], device=device)
        loss_fn = get_loss_function(
            loss_name=loss_name,
            alpha=alpha,
            class_weights=class_weights if loss_name == "weighted_ce_dice" else None
        )

        optimizer = optim.Adam(model.parameters(), lr=lr)
        scheduler = lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=epochs,
            eta_min=eta_min
        )

        fold_train_losses = []
        fold_val_losses   = []
        fold_val_dices    = []

        for epoch in range(1, epochs + 1):
            if use_ohem:
                model.eval()
                dice_list = []
                train_loader_no_shuffle = DataLoader(train_patch_ds, batch_size=1, shuffle=False)
                with torch.no_grad():
                    for i, (inputs, labels) in enumerate(train_loader_no_shuffle):
                        inputs, labels = inputs.to(device), labels.to(device)
                        logits = model(inputs)
                        pred = torch.argmax(logits, dim=1)
                        pred_np = pred.cpu().numpy()[0]
                        labels_np = labels.cpu().numpy()[0]
                        dice = dice_coefficient(pred_np, labels_np)
                        dice_list.append((dice, i))
                weights = [1.0 - dice for dice, _ in dice_list]
                min_w = min(weights)
                max_w = max(weights)
                if max_w != min_w:
                    normalized_weights = [(w - min_w)/(max_w - min_w) + 0.5 for w in weights]
                else:
                    normalized_weights = [1.0]*len(weights)
                sampler = WeightedRandomSampler(
                    weights=normalized_weights,
                    num_samples=len(train_patch_ds),
                    replacement=True
                )
                weighted_train_loader = DataLoader(
                    train_patch_ds,
                    batch_size=batch_size,
                    sampler=sampler,
                    drop_last=True
                )
                print(f"Epoch {epoch}: Using weighted sampling (OHEM) for patch-based training")
                train_loss = train_one_epoch(model, weighted_train_loader, optimizer, loss_fn, device=device)
            else:
                train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device=device)

            val_metrics = validate_one_epoch(model, val_loader, loss_fn, device=device)
            scheduler.step()

            fold_train_losses.append(train_loss)
            fold_val_losses.append(val_metrics['val_loss'])
            fold_val_dices.append(val_metrics['val_dice'])

            current_lr = scheduler.get_last_lr()[0]
            log_str = (f"[FOLD {fold_idx+1}] Epoch {epoch}/{epochs} => "
                       f"Train loss={train_loss:.4f}, Val loss={val_metrics['val_loss']:.4f}, "
                       f"Val Dice={val_metrics['val_dice']:.4f}, lr={current_lr:.6f}")
            if compute_surface_metrics:
                log_str += (f", val_masd={val_metrics.get('val_masd', 0):.4f}, "
                            f"val_nsd={val_metrics.get('val_nsd', 0):.4f}")
            print(log_str)

            wandb_log = {
                'fold': fold_idx + 1,
                'epoch': epoch,
                'train_loss': train_loss,
                'val_loss': val_metrics['val_loss'],
                'val_dice': val_metrics['val_dice'],
                'lr': current_lr
            }
            if compute_surface_metrics:
                wandb_log.update({
                    'val_masd': val_metrics.get('val_masd', 0),
                    'val_nsd':  val_metrics.get('val_nsd', 0),
                })
            wandb.log(wandb_log)

        # Pro validaci provedeme full-volume inference
        val_results = infer_full_volume_on_val_set_full(
            model=model,
            val_indices=val_indices,
            adc_files=adc_files_full,
            z_files=z_files_full,
            label_files=lab_files_full,
            adc_folder=adc_folder,
            z_folder=z_folder,
            label_folder=label_folder,
            device=device
        )

        all_val_losses.append(fold_val_losses[-1])
        all_val_dices.append(fold_val_dices[-1])

    mean_val_loss = np.mean(all_val_losses)
    mean_val_dice = np.mean(all_val_dices)
    print("\n=== SUBJECT-WISE PATCH-BASED CROSS-VALIDATION RESULTS ===")
    for i in range(n_folds):
        print(f" Fold {i+1} => val_loss={all_val_losses[i]:.4f}, val_dice={all_val_dices[i]:.4f}")
    print(f" Overall => mean_val_loss={mean_val_loss:.4f}, mean_val_dice={mean_val_dice:.4f}")

    return mean_val_loss, mean_val_dice

def run_cross_validation_moe(
    adc_folder,
    z_folder,
    label_folder,
    n_folds,
    lr,
    eta_min,
    use_augmentation=True,
    device="cuda",
    epochs=50,
    batch_size=1,
    allowed_patient_ids=None,
    threshold=80
):
    full_dataset_main_no_aug = BONBID3DFullVolumeDataset(
        adc_folder=adc_folder,
        z_folder=z_folder,
        label_folder=label_folder,
        augment=False,
        allowed_patient_ids=None
    )
    full_dataset_main_aug = BONBID3DFullVolumeDataset(
        adc_folder=adc_folder,
        z_folder=z_folder,
        label_folder=label_folder,
        augment=use_augmentation,
        allowed_patient_ids=None
    )
    full_dataset_expert_no_aug = BONBID3DFullVolumeDataset(
        adc_folder=adc_folder,
        z_folder=z_folder,
        label_folder=label_folder,
        augment=False,
        allowed_patient_ids=allowed_patient_ids
    )
    full_dataset_expert_aug = BONBID3DFullVolumeDataset(
        adc_folder=adc_folder,
        z_folder=z_folder,
        label_folder=label_folder,
        augment=use_augmentation,
        allowed_patient_ids=allowed_patient_ids
    )

    # Pro cross-validation rozdělení využijeme hlavní dataset (bez filtru)
    subject_to_indices = {}
    for i, fname in enumerate(full_dataset_main_no_aug.adc_files):
        subj_id = get_subject_id_from_filename(fname)
        if subj_id not in subject_to_indices:
            subject_to_indices[subj_id] = []
        subject_to_indices[subj_id].append(i)

    all_subjects = list(subject_to_indices.keys())
    random.shuffle(all_subjects)
    num_subjects = len(all_subjects)
    fold_size = num_subjects // n_folds

    all_val_dices = []

    for fold_idx in range(n_folds):
        print(f"=== MOE FOLD {fold_idx+1}/{n_folds} ===")
        fold_val_start = fold_idx * fold_size
        fold_val_end = (fold_idx+1)*fold_size if fold_idx < n_folds-1 else num_subjects
        val_subjects = all_subjects[fold_val_start:fold_val_end]
        train_subjects = list(set(all_subjects) - set(val_subjects))

        val_indices = []
        for subj_id in val_subjects:
            indices = subject_to_indices[subj_id]
            if extended_dataset:
                for idx in indices:
                    adc_fname = full_dataset_main_no_aug.adc_files[idx]
                    if "_orig_" in adc_fname.lower():
                        val_indices.append(idx)
            else:
                val_indices.extend(indices)

        # --- VÝBĚR TRAINING INDEXŮ PRO HLAVNÍ MODEL ---
        train_indices_main = []
        for subj_id in train_subjects:
            indices = subject_to_indices[subj_id]
            train_indices_main.extend(indices)

        # --- VÝBĚR INDEXŮ PRO EXPERT MODEL ---
        # Pouze z expertního datasetu (který je již filtrovaný podle allowed_patient_ids)
        # a pouze pro tréninkové subjekty
        expert_indices = []
        for i, fname in enumerate(full_dataset_expert_no_aug.adc_files):
            subj = get_subject_id_from_filename(fname)
            if subj in train_subjects:
                expert_indices.append(i)

        print(f"  Fold {fold_idx+1}:")
        print(f"     Val subjects: {len(val_subjects)}, Val indices: {len(val_indices)}")
        print(f"     Train subjects (main): {len(train_subjects)}, Train indices (main): {len(train_indices_main)}")
        print(f"     Expert indices: {len(expert_indices)}")

        # Vytvoříme subsety
        train_ds_main = Subset(full_dataset_main_aug, train_indices_main)
        val_ds = Subset(full_dataset_main_no_aug, val_indices)
        train_ds_expert = Subset(full_dataset_expert_aug, expert_indices)

        main_loader = DataLoader(train_ds_main, batch_size=batch_size, shuffle=True)
        expert_loader = DataLoader(train_ds_expert, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)

        main_model = create_model(model_name=model_name, in_channels=2, out_channels=2).to(device)
        expert_model = create_model(model_name=expert_model_name, in_channels=2, out_channels=2).to(device)

        loss_fn_main = get_loss_function(loss_name=loss_name, alpha=0.0)
        loss_fn_expert = get_loss_function(loss_name=expert_loss_name, alpha=0.0)

        optimizer_main = torch.optim.Adam(main_model.parameters(), lr=lr)
        optimizer_expert = torch.optim.Adam(expert_model.parameters(), lr=expert_lr)
        scheduler_main = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_main, T_max=epochs, eta_min=eta_min)
        scheduler_expert = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_expert, T_max=epochs, eta_min=expert_eta_min)

        for epoch in range(1, epochs+1):
            main_model.train()
            main_running_loss = 0.0
            for inputs, labels in main_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                optimizer_main.zero_grad()
                logits = main_model(inputs)
                loss = loss_fn_main(logits, labels)
                loss.backward()
                optimizer_main.step()
                main_running_loss += loss.item()
            main_epoch_loss = main_running_loss / len(main_loader)

            expert_model.train()
            expert_running_loss = 0.0
            if len(expert_loader) > 0:
                for inputs, labels in expert_loader:
                    inputs, labels = inputs.to(device), labels.to(device)
                    optimizer_expert.zero_grad()
                    logits = expert_model(inputs)
                    loss = loss_fn_expert(logits, labels)
                    loss.backward()
                    optimizer_expert.step()
                    expert_running_loss += loss.item()
                expert_epoch_loss = expert_running_loss / len(expert_loader)
            else:
                expert_epoch_loss = 0.0

            scheduler_main.step()
            scheduler_expert.step()

            main_model.eval()
            expert_model.eval()

            overall_dice_sum = 0.0
            overall_masd_sum = 0.0
            overall_nsd_sum = 0.0
            overall_count = 0

            main_dice_sum = 0.0
            main_masd_sum = 0.0
            main_nsd_sum = 0.0
            main_count = 0

            expert_dice_sum = 0.0
            expert_masd_sum = 0.0
            expert_nsd_sum = 0.0
            expert_count = 0

            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                with torch.no_grad():
                    logits_main = main_model(inputs)
                    preds_main = torch.argmax(logits_main, dim=1)
                    for i in range(preds_main.shape[0]):
                        pred_main = preds_main[i].cpu().numpy()
                        fg_count = np.sum(pred_main == 1)
                        lab_np = labels[i].cpu().numpy()

                        if fg_count < threshold and fg_count > 3:
                            with torch.no_grad():
                                logits_expert = expert_model(inputs[i].unsqueeze(0))
                            pred_final = torch.argmax(logits_expert, dim=1).cpu().numpy()[0]
                            model_used = 'expert'
                        else:
                            pred_final = pred_main
                            model_used = 'main'

                        dice_val = dice_coefficient(pred_final, lab_np)
                        masd_val = compute_masd(pred_final, lab_np, spacing=(1,1,1), sampling_ratio=0.5)
                        nsd_val = compute_nsd(pred_final, lab_np, spacing=(1,1,1), tau=1.0, sampling_ratio=0.5)
                        overall_dice_sum += dice_val
                        overall_masd_sum += masd_val
                        overall_nsd_sum += nsd_val
                        overall_count += 1
                        if model_used == 'main':
                            main_dice_sum += dice_val
                            main_masd_sum += masd_val
                            main_nsd_sum += nsd_val
                            main_count += 1
                        else:
                            expert_dice_sum += dice_val
                            expert_masd_sum += masd_val
                            expert_nsd_sum += nsd_val
                            expert_count += 1

            overall_dice = overall_dice_sum / overall_count if overall_count > 0 else 0.0
            overall_masd = overall_masd_sum / overall_count if overall_count > 0 else 0.0
            overall_nsd = overall_nsd_sum / overall_count if overall_count > 0 else 0.0

            main_dice = main_dice_sum / main_count if main_count > 0 else 0.0
            main_masd = main_masd_sum / main_count if main_count > 0 else 0.0
            main_nsd = main_nsd_sum / main_count if main_count > 0 else 0.0

            expert_dice = expert_dice_sum / expert_count if expert_count > 0 else 0.0
            expert_masd = expert_masd_sum / expert_count if expert_count > 0 else 0.0
            expert_nsd = expert_nsd_sum / expert_count if expert_count > 0 else 0.0

            print(f"Epoch {epoch}: Celkově -> Dice: {overall_dice:.4f}, MASD: {overall_masd:.4f}, NSD: {overall_nsd:.4f}")
            print(f"Epoch {epoch}: Hlavní model -> Dice: {main_dice:.4f}, MASD: {main_masd:.4f}, NSD: {main_nsd:.4f} (zpracováno {main_count} vzorků)")
            print(f"Epoch {epoch}: Expertní model -> Dice: {expert_dice:.4f}, MASD: {expert_masd:.4f}, NSD: {expert_nsd:.4f} (zpracováno {expert_count} vzorků)")

            wandb.log({
                'val_overall_dice': overall_dice,
                'val_overall_masd': overall_masd,
                'val_overall_nsd': overall_nsd,
                'val_main_dice': main_dice,
                'val_main_masd': main_masd,
                'val_main_nsd': main_nsd,
                'val_expert_dice': expert_dice,
                'val_expert_masd': expert_masd,
                'val_expert_nsd': expert_nsd,
            })

            # ---- NOVÁ INFERENCE PO 10 EPOCHÁCH ----
            if epoch % 10 == 0:
                print(f"Performing full-volume MOE inference at epoch {epoch}...")
                val_results = infer_full_volume_on_val_set_moe(
                    main_model=main_model,
                    expert_model=expert_model,
                    val_indices=val_indices,
                    adc_files=full_dataset_main_no_aug.adc_files,
                    z_files=full_dataset_main_no_aug.z_files,
                    lab_files=full_dataset_main_no_aug.lab_files,
                    adc_folder=adc_folder,
                    z_folder=z_folder,
                    label_folder=label_folder,
                    device=device,
                    threshold=threshold
                )

    return main_model, expert_model

import re

def get_patient_numeric_id(filename: str):
    """
    Ze souboru např. "MGHNICU_010-VISIT_01-ADC_ss_orig_ADC.mha" extrahuje číslo 10.
    """
    match = re.search(r"MGHNICU_(\d+)", filename)
    if match:
        return int(match.group(1))
    return None

# Pokud již máte get_subject_id_from_filename, můžete ji ponechat:
def get_subject_id_from_filename(filename: str):
    match = re.search(r"(MGHNICU_\d+)", filename)
    if match:
        return match.group(1)
    return "unknown"

def get_base_id(filename: str):
    filename_lower = filename.lower()
    if '_orig_' in filename_lower:
        return filename_lower.split('_orig_')[0]
    else:
        return re.sub(r'_aug\d+.*', '', filename_lower)




def extract_patient_id(filepath):
    """
    Vyhledá v názvu souboru vzor 'MGHNICU_XXX-' a vrátí XXX jako string.
    Např. z 'MGHNICU_010-VISIT_01-ADC_ss.mha' získá '010'.
    Pokud nic nenajde, vrací 'unknown'.
    """
    match = re.search(r"MGHNICU_(\d+)-", filepath)
    if match:
        return match.group(1)
    else:
        return "unknown"


def save_segmentation_to_mha(seg_np, ref_sitk, out_path):
    """
    Uloží numpy masku seg_np do MHA souboru.
    'ref_sitk' je referenční SimpleITK obraz, abychom mohli
    zachovat spacing/origin/direction.
    """
    seg_sitk = sitk.GetImageFromArray(seg_np.astype(np.uint8))
    seg_sitk.CopyInformation(ref_sitk)
    sitk.WriteImage(seg_sitk, out_path)


##PRO FULL VOLUME TRENINK
def infer_full_volume_on_val_set_full(model,
                                      val_indices,
                                      adc_files,
                                      z_files,
                                      label_files,
                                      adc_folder,
                                      z_folder,
                                      label_folder,
                                      device="cuda"):
    model.eval()
    results = []
    tta_transforms = get_tta_transforms(angle_max=TTA_ANGLE_MAX) if USE_TTA else None

    with torch.no_grad():
        for idx in val_indices:
            adc_path   = os.path.join(adc_folder,   adc_files[idx])
            zadc_path  = os.path.join(z_folder,     z_files[idx])
            lab_path   = os.path.join(label_folder, label_files[idx])

            patient_id = extract_patient_id(adc_files[idx])
            adc_sitk = sitk.ReadImage(adc_path)
            z_sitk   = sitk.ReadImage(zadc_path)
            lab_sitk = sitk.ReadImage(lab_path)

            adc_np = sitk.GetArrayFromImage(adc_sitk).astype(np.float32)
            z_np   = sitk.GetArrayFromImage(z_sitk).astype(np.float32)
            lab_np = sitk.GetArrayFromImage(lab_sitk)

            input_vol = np.stack([adc_np, z_np], axis=0)  # (2, D, H, W)
            input_tensor = torch.from_numpy(input_vol).unsqueeze(0).to(device).float()  # (1, 2, D, H, W)

            if USE_TTA:
                avg_probs = 0
                for transform in tta_transforms:
                    aug_vol = apply_tta_transform(input_vol, transform)
                    aug_tensor = torch.from_numpy(aug_vol).unsqueeze(0).to(device).float()
                    if training_mode == "patch":
                        pred_logits = sliding_window_inference(aug_tensor, patch_size, batch_size, model, overlap=0.25)
                    else:
                        pred_logits = model(aug_tensor)
                    softmax = torch.nn.Softmax(dim=1)
                    probs = softmax(pred_logits)
                    probs_np = probs.cpu().numpy()[0]
                    inv_probs = invert_tta_transform(probs_np, transform)
                    avg_probs += inv_probs
                avg_probs /= len(tta_transforms)
                pred_np = np.argmax(avg_probs, axis=0)
            else:
                if training_mode == "patch":
                    pred_logits = sliding_window_inference(input_tensor, patch_size, batch_size, model, overlap=0.25)
                else:
                    pred_logits = model(input_tensor)
                pred_np = torch.argmax(pred_logits, dim=1).cpu().numpy()[0]

            dsc = dice_coefficient(pred_np, lab_np)
            masd_val = compute_masd(pred_np, lab_np, spacing=(1,1,1), sampling_ratio=0.5)
            nsd_val  = compute_nsd(pred_np, lab_np, spacing=(1,1,1), sampling_ratio=0.5)
            print(f"   DSC for idx={idx} => {dsc:.4f}, MASD={masd_val:.4f}, NSD={nsd_val:.4f}")

            results.append({
                'val_idx': idx,
                'adc_path': adc_path,
                'zadc_path': zadc_path,
                'label_path': lab_path,
                'patient_id': patient_id,
                'prediction': pred_np,
                'lab_np': lab_np,
                'dice': dsc,
                'masd': masd_val,
                'nsd': nsd_val
            })

    num_to_save = min(len(results), len(results))
    random_5 = random.sample(results, num_to_save)
    os.makedirs(out_dir_random, exist_ok=True)

    print(f"\nSaving {num_to_save} random segmentations to {out_dir_random} ...")
    for i, res in enumerate(random_5, start=1):
        patient_id = res['patient_id']
        dsc_rand   = res['dice']
        masd_rand  = res['masd']
        nsd_rand   = res['nsd']
        adc_ref_sitk = sitk.ReadImage(res['adc_path'])

        out_path_rand = os.path.join(
            out_dir_random,
            f"random{i}_pac{patient_id}_dice{dsc_rand:.3f}_masd{masd_rand:.3f}_nsd{nsd_rand:.3f}.mha"
        )

        save_segmentation_to_mha(res['prediction'], adc_ref_sitk, out_path_rand)
        print(f"   [RANDOM {i}] Saved => {out_path_rand}")

    return results


def infer_full_volume_on_val_set_moe(main_model, expert_model, val_indices, adc_files, z_files, lab_files, adc_folder, z_folder, label_folder, device="cuda", threshold=80):
    main_model.eval()
    expert_model.eval()
    results = []
    os.makedirs(out_dir_random, exist_ok=True)

    for idx in val_indices:
        adc_path   = os.path.join(adc_folder, adc_files[idx])
        zadc_path  = os.path.join(z_folder, z_files[idx])
        lab_path   = os.path.join(label_folder, lab_files[idx])
        patient_id = extract_patient_id(adc_files[idx])

        # Načtení originálních dat
        adc_sitk = sitk.ReadImage(adc_path)
        z_sitk   = sitk.ReadImage(zadc_path)
        lab_sitk = sitk.ReadImage(lab_path)
        adc_np   = sitk.GetArrayFromImage(adc_sitk).astype(np.float32)
        z_np     = sitk.GetArrayFromImage(z_sitk).astype(np.float32)
        lab_np   = sitk.GetArrayFromImage(lab_sitk)

        # Sestavení vstupního volumenu (tvar: (2, D, H, W))
        input_vol = np.stack([adc_np, z_np], axis=0)
        input_tensor = torch.from_numpy(input_vol).unsqueeze(0).to(device).float()

        with torch.no_grad():
            logits_main = main_model(input_tensor)
            pred_main = torch.argmax(logits_main, dim=1).cpu().numpy()[0]
        fg_count = np.sum(pred_main == 1)

        # Pokud má výstup hlavního modelu méně než threshold voxelů (a je >1), použijeme expertní model
        if fg_count < threshold and fg_count > 1:
            with torch.no_grad():
                logits_expert = expert_model(input_tensor)
                pred_final = torch.argmax(logits_expert, dim=1).cpu().numpy()[0]
        else:
            pred_final = pred_main

        dsc = dice_coefficient(pred_final, lab_np)
        masd_val = compute_masd(pred_final, lab_np, spacing=(1,1,1), sampling_ratio=0.5)
        nsd_val = compute_nsd(pred_final, lab_np, spacing=(1,1,1), sampling_ratio=0.5)
        print(f"   MOE DSC for idx={idx} => {dsc:.4f}, MASD={masd_val:.4f}, NSD={nsd_val:.4f}")

        results.append({
            'val_idx': idx,
            'adc_path': adc_path,
            'zadc_path': zadc_path,
            'label_path': lab_path,
            'patient_id': patient_id,
            'prediction': pred_final,
            'lab_np': lab_np,
            'dice': dsc,
            'masd': masd_val,
            'nsd': nsd_val
        })

    # Uložení několika náhodných segmentací do výstupní složky
    num_to_save = min(len(results), 5)
    random_samples = random.sample(results, num_to_save)
    for i, res in enumerate(random_samples, start=1):
         adc_ref_sitk = sitk.ReadImage(res['adc_path'])
         out_path_rand = os.path.join(out_dir_random, f"MOE_random{i}_pac{res['patient_id']}_dice{res['dice']:.3f}_masd{res['masd']:.3f}_nsd{res['nsd']:.3f}.mha")
         save_segmentation_to_mha(res['prediction'], adc_ref_sitk, out_path_rand)
         print(f"   [MOE RANDOM {i}] Saved => {out_path_rand}")
    return results


def save_segmentation_with_patient_id(results, output_dir):
    """
    Uloží segmentace do souborů, jejichž názvy obsahují ID pacienta, DICE skóre, MASD a NSD.
    """
    os.makedirs(output_dir, exist_ok=True)
    for result in results:
        patient_id = result['patient_id']
        dice_score = result['dice']
        masd_score = result['masd']
        nsd_score  = result['nsd']
        out_path = os.path.join(output_dir, f"patient_{patient_id}_dice{dice_score:.3f}_masd{masd_score:.3f}_nsd{nsd_score:.3f}.mha")

        ref_sitk = sitk.ReadImage(result['adc_path'])  # Referenční SimpleITK obraz
        save_segmentation_to_mha(result['prediction'], ref_sitk, out_path)
        print(f"Saved segmentation: {out_path}")


def save_segmentation_to_mha(seg_np, ref_sitk, out_path):
    """
    Uloží numpy masku `seg_np` do MHA souboru.
    `ref_sitk` je referenční SimpleITK obraz, abychom mohli
    přenést spacing/origin/direction.
    """
    seg_sitk = sitk.GetImageFromArray(seg_np.astype(np.uint8))
    seg_sitk.CopyInformation(ref_sitk)
    sitk.WriteImage(seg_sitk, out_path)

from scipy.ndimage import label

def remove_small_regions(mask, min_size=2):
    """
    Odstraní z binární masky (0 = background, 1 = foreground)
    všechny spojené komponenty, které mají méně než min_size voxelů.

    Args:
        mask (np.ndarray): Binární maska (např. predikce modelu), tvar (D, H, W).
        min_size (int): Minimální počet voxelů, které musí mít komponenta, aby zůstala.

    Returns:
        np.ndarray: Upravená maska se odstraněnými malými komponentami.
    """
    # Označíme všechny spojené komponenty
    labeled_mask, num_features = label(mask)
    # Vytvoříme novou masku se stejným tvarem, kde budeme uchovávat jen komponenty s dostatečnou velikostí
    cleaned_mask = np.zeros_like(mask)

    # Projdeme všechny nalezené komponenty (číslovány od 1 do num_features)
    for region_label in range(1, num_features + 1):
        # Vytvoříme masku pro aktuální komponentu
        region = (labeled_mask == region_label)
        # Pokud je velikost komponenty (počet voxelů) větší nebo rovna min_size, ponecháme ji
        if np.sum(region) >= min_size:
            cleaned_mask[region] = 1
    return cleaned_mask

def apply_tta_transform(volume, transform):
    """
    Aplikuje transformaci na full-volume data.
    Očekává vstupní pole volume se tvarem (C, D, H, W).

    Transformace (dictionary) může obsahovat:
      - "flip": bool – zda provést horizontální flip (podél osy 2, tj. u každého kanálu np.flip(ch, axis=2))
      - "rotation": buď None, nebo slovník s klíči "angle" (v°) a "axes" (příklad: (0,1), (0,2) nebo (1,2))
    """
    vol_transformed = np.copy(volume)
    if transform.get("flip", False):
        # Aplikovat flip na každý kanál
        vol_transformed = np.array([np.flip(ch, axis=2) for ch in vol_transformed])

    rotation = transform.get("rotation", None)
    if rotation is not None:
        angle = rotation.get("angle", 0)
        axes  = rotation.get("axes", (0, 1))
        # Aplikovat rotaci na každý kanál – stejná volba parametrů jako v tréninku:
        vol_transformed = np.array([rotate(ch, angle=angle, axes=axes, reshape=False, order=1, mode='nearest')
                                    for ch in vol_transformed])
    return vol_transformed

def invert_tta_transform(volume, transform):
    """
    Inverzuje aplikovanou transformaci na predikovaných pravděpodobnostních mapách.
    Očekává vstupní pole volume se tvarem (num_classes, D, H, W).

    Inverzní operace:
      1) Pokud byla aplikována rotace, provedeme rotaci o opačném úhlu.
      2) Pokud byl proveden flip, provedeme flip (flip je samo-inverzní).
    """
    vol_inverted = np.copy(volume)
    rotation = transform.get("rotation", None)
    if rotation is not None:
        angle = rotation.get("angle", 0)
        axes  = rotation.get("axes", (0, 1))
        vol_inverted = np.array([rotate(ch, angle=-angle, axes=axes, reshape=False, order=1, mode='nearest')
                                 for ch in vol_inverted])
    if transform.get("flip", False):
        vol_inverted = np.array([np.flip(ch, axis=2) for ch in vol_inverted])
    return vol_inverted

def get_tta_transforms(angle_max=TTA_ANGLE_MAX):
    """
    Vytvoří seznam předdefinovaných TTA transformací, které odpovídají vámi používaným augmentacím.
    Kombinujeme možnost flipu (True/False) a rotaci:
      - žádná rotace (None)
      - rotace o +angle_max a -angle_max pro každý z párů os: (0,1), (0,2), (1,2)

    Vrací seznam slovníků s transformacemi.
    """
    tta_transforms = []
    flips = [False, True]
    rotations = [None,
                 {"angle": angle_max,  "axes": (0, 1)},
                 {"angle": -angle_max, "axes": (0, 1)},
                 {"angle": angle_max,  "axes": (0, 2)},
                 {"angle": -angle_max, "axes": (0, 2)},
                 {"angle": angle_max,  "axes": (1, 2)},
                 {"angle": -angle_max, "axes": (1, 2)}]

    for flip in flips:
        for rot in rotations:
            tta_transforms.append({"flip": flip, "rotation": rot})
    return tta_transforms

def tta_forward(model, input_tensor, device, tta_transforms):
    """
    Provede inference s TTA pro jeden vstupní full-volume vzorek.

    Parametry:
      - model: váš trénovaný model
      - input_tensor: Torch tensor se tvarem (1, C, D, H, W)
      - device: cílové zařízení ("cuda" nebo "cpu")
      - tta_transforms: seznam transformací (dostanete pomocí get_tta_transforms)

    Vrací průměrnou pravděpodobnostní mapu (numpy pole se tvarem (num_classes, D, H, W)).
    """
    model.eval()
    softmax = torch.nn.Softmax(dim=1)

    input_np = input_tensor.cpu().numpy()[0]  # tvar: (C, D, H, W)
    accumulated_probs = None

    for transform in tta_transforms:
        aug_vol = apply_tta_transform(input_np, transform)
        aug_tensor = torch.from_numpy(aug_vol).unsqueeze(0).to(device).float()
        logits = model(aug_tensor)
        probs = softmax(logits)
        probs_np = probs.cpu().numpy()[0]
        inv_probs = invert_tta_transform(probs_np, transform)

        if accumulated_probs is None:
            accumulated_probs = inv_probs
        else:
            accumulated_probs += inv_probs

    avg_probs = accumulated_probs / len(tta_transforms)
    return avg_probs

from datetime import datetime
def initialize_wandb_run():
    run_id = datetime.now().strftime("%Y%m%d%H%M%S")

    if extended_dataset:
      datatype = "extended"
    else:
      datatype = "base"

    run_name = (
        f"{model_name} | "
        f"lr={lr} | "
        f"Loss={loss_name} | "
        f"Train mode={training_mode} | "
        f"Data ={datatype} | "
        f"special=OFF OHEM"
    )

    wandb.init(
        project="HIE-segmentation",
        entity="turekmat-czech-technical-university-in-prague",
        name=run_name,
        settings=wandb.Settings(init_timeout=120)
    )

    wandb.config.update({
        "alpha": alpha,
        "bg_weight": bg_weight,
        "fg_weight": fg_weight,
        "epochs": epochs,
        "lr": lr,
        "patch_size": patch_size,
        "step_size": step_size,
        "gamma": gamma,
        "use_data_augmentation": use_data_augmentation,
        "use_normalization": use_normalization,
        "n_folds": n_folds,
        "batch_size": batch_size,
        "model_name": model_name,
        "in_channels": in_channels,
        "out_channels": out_channels,
        "compute_surface_metrics": compute_surface_metrics  # Přidáno
    })
    print("Wandb initialized")

def main_training():
    print("Using model: ", model_name)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Using device:", device)

    initialize_wandb_run()

    prepare_preprocessed_data(
        adc_folder,
        z_folder,
        label_folder,
        preprocessed_adc_folder,
        preprocessed_z_folder,
        preprocessed_label_folder,
        normalize=use_normalization,
        allow_normalize_spacing=allow_normalize_spacing
    )

    if training_mode == "full_volume":
        mean_val_loss, mean_val_dice = run_cross_validation_full_volume(
            adc_folder=preprocessed_adc_folder,
            z_folder=preprocessed_z_folder,
            label_folder=preprocessed_label_folder,
            n_folds=n_folds,
            device=device,
            epochs=epochs,
            batch_size=batch_size,
            lr=lr,
            eta_min=eta_min,
            use_augmentation=use_data_augmentation
        )
    elif training_mode == "patch":
        mean_val_loss, mean_val_dice = run_cross_validation_patch_based(
            adc_folder=preprocessed_adc_folder,
            z_folder=preprocessed_z_folder,
            label_folder=preprocessed_label_folder,
            n_folds=n_folds,
            device=device,
            epochs=epochs,
            batch_size=batch_size,
            lr=lr,
            eta_min=eta_min,
            use_augmentation=use_data_augmentation,
            patches_per_volume=patch_per_volume,
            patch_size=patch_size
        )
    else:
        raise ValueError("Unsupported training_mode. Use 'full_volume' or 'patch'.")

    print(f"Training finished. Mean val_loss={mean_val_loss:.4f}, mean val_dice={mean_val_dice:.4f}")
    wandb.finish()

if __name__ == "__main__":
    initialize_wandb_run()

    prepare_preprocessed_data(
        adc_folder,
        z_folder,
        label_folder,
        preprocessed_adc_folder,
        preprocessed_z_folder,
        preprocessed_label_folder,
        normalize=use_normalization,
        allow_normalize_spacing=allow_normalize_spacing
    )

    low_lesion_ids = {458, 442, 209, 432, 215, 231, 329, 144, 309, 71, 362, 331, 448, 346, 452, 453, 370, 312, 377, 376, 447, 359, 275, 263, 403, 290, 444, 393, 457, 348, 336, 454, 439, 358, 455, 449}

    if use_moe:
        main_model, expert_model = run_cross_validation_moe(
            adc_folder=preprocessed_adc_folder,
            z_folder=preprocessed_z_folder,
            label_folder=preprocessed_label_folder,
            n_folds=n_folds,
            lr=lr,
            eta_min=eta_min,
            use_augmentation=use_data_augmentation,
            device="cuda",
            epochs=epochs,
            batch_size=batch_size,
            allowed_patient_ids=low_lesion_ids,
        )
    else:
        if training_mode == "full_volume":
            mean_val_loss, mean_val_dice = run_cross_validation_full_volume(
                adc_folder=preprocessed_adc_folder,
                z_folder=preprocessed_z_folder,
                label_folder=preprocessed_label_folder,
                n_folds=n_folds,
                device="cuda",
                epochs=epochs,
                batch_size=batch_size,
                lr=lr,
                eta_min=eta_min,
                use_augmentation=use_data_augmentation
            )
        elif training_mode == "patch":
            mean_val_loss, mean_val_dice = run_cross_validation_patch_based(
                adc_folder=preprocessed_adc_folder,
                z_folder=preprocessed_z_folder,
                label_folder=preprocessed_label_folder,
                n_folds=n_folds,
                device="cuda",
                epochs=epochs,
                batch_size=batch_size,
                lr=lr,
                eta_min=eta_min,
                use_augmentation=use_data_augmentation,
                patches_per_volume=patch_per_volume,
                patch_size=patch_size
            )
        print(f"Klasická CV: Mean val_loss={mean_val_loss:.4f}, Mean val_dice={mean_val_dice:.4f}")

    wandb.finish()